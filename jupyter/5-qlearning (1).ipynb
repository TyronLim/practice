{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553078ed-eb92-4441-83ef-5077d60950b0",
   "metadata": {},
   "source": [
    "# 5강 : q-learning\n",
    "\n",
    "Supermario, Cartpole, Frozen Lake\n",
    "Credit by 정원석 ( Jay Wonseok Jung ) 대우교수 - HISS대표, 서울사이버대학교 대우교수\n",
    "\n",
    "안녕하세요, 학생 여러분! 이번 시간에는 인공지능 에이전트가 게임을 학습하는 방법 중 하나인 Q-learning에 대해 배워보도록 하겠습니다.\n",
    "\n",
    "우리가 살펴볼 게임은 바로 슈퍼 마리오 브라더스입니다. 마리오는 장애물을 피하고, 적을 물리치면서 목적지까지 도달해야 하는 클래식한 플랫폼 게임이죠. 우리는 마리오 게임 환경을 라이브러리를 통해 구현할 것입니다.\n",
    "\n",
    "Q-learning은 강화 학습의 한 종류로, 에이전트가 환경과 상호작용하면서 보상을 최대화하는 방향으로 학습하는 알고리즘입니다. 에이전트는 현재 상태에서 가능한 행동들의 가치(Q-value)를 업데이트하면서, 장기적인 관점에서 최선의 행동 정책을 찾아갑니다.\n",
    "\n",
    "마리오 에이전트는 게임 화면을 보고 현재 상태(시각 정보가 아닌 좌표 등)를 인식한 뒤, Q-learning을 통해 어떤 행동을 취해야 할지 판단하게 됩니다. 점프를 해야 할지, 앞으로 달려야 할지, 적을 밟아야 할지 등을 학습하는 거죠. 에이전트가 목표 지점에 도달하면 보상을 받고, 실패하면 패널티를 받습니다. 이렇게 시행착오를 거듭하면서 에이전트는 점차 게임을 잘 플레이하는 방법을 익히게 될 것입니다.\n",
    "\n",
    "자, 그럼 이제 마리오 에이전트를 학습시키기 위한 Q-learning 코드를 함께 살펴보도록 할까요? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c856be-ac06-4cc6-8bbc-76e95ac82252",
   "metadata": {},
   "source": [
    "\n",
    "# 1.Step 1 설치\n",
    "\n",
    "필요한 라이브러리 설치\n",
    "주피터 노트북에서는 `!`를 사용하여 터미널 명령을 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016a4759-9133-4aeb-96a4-80236635dcbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\tyron\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gym_super_mario_bros in c:\\users\\tyron\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes_py in c:\\users\\tyron\\anaconda3\\lib\\site-packages (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes_py) (0.4.6)\n",
      "Requirement already satisfied: gym in c:\\users\\tyron\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: nes_py in c:\\users\\tyron\\anaconda3\\lib\\site-packages (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes_py) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes_py) (0.4.6)\n",
      "Requirement already satisfied: gym_super_mario_bros in c:\\users\\tyron\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym_super_mario_bros) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym_super_mario_bros) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade gym\n",
    "!pip3 install gym_super_mario_bros nes_py\n",
    "!pip3 install gym\n",
    "!pip3 install --upgrade nes_py\n",
    "!pip3 install gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9474c-0ac6-45a9-a286-ae633d9140d3",
   "metadata": {},
   "source": [
    "# Step 2 필요한 라이브러리 임포트\n",
    "\n",
    "\n",
    "gym: 강화학습 환경을 제공하는 라이브러리\n",
    "gym_super_mario_bros: Super Mario Bros 게임 환경\n",
    "JoypadSpace, SIMPLE_MOVEMENT: 조작과 행동을 단순화하기 위한 도구들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13b88b0-b83e-46ab-b30e-9722d8a09204",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c27e9-255a-441a-a173-e0409f871b72",
   "metadata": {},
   "source": [
    "# 3단계: q-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acebc7-4d74-4d2a-a1f1-fa64666db8ad",
   "metadata": {},
   "source": [
    "강화학습에서 에이전트는 환경과 상호작용하면서 보상을 최대화하는 방향으로 학습합니다. \n",
    "\n",
    "Q-Learning은 강화학습의 대표적인 알고리즘 중 하나로, Q-Value라는 값을 기반으로 의사결정을 합니다.\n",
    "\n",
    "Q-Value는 특정 상태(state)에서 특정 행동(action)을 취했을 때 얻을 수 있는 기대 보상의 합을 나타냅니다. \n",
    "\n",
    "에이전트는 현재 상태에서 가능한 모든 행동의 Q-Value를 비교하여 가장 높은 Q-Value를 가진 행동을 선택하게 됩니다.\n",
    "\n",
    "Q-Learning에서는 Q-Table이라는 테이블을 사용하여 각 상태-행동 쌍의 Q-Value를 저장합니다.\n",
    "\n",
    "에이전트가 환경과 상호작용하면서 얻은 경험을 바탕으로 Q-Table을 업데이트하게 되는데, 이때 사용되는 것이 Q-Learning 업데이트 규칙입니다.\n",
    "\n",
    "\n",
    "Q(s,a) ← (1 - α) * Q(s,a) + α * [r + γ * max(Q(s',a'))]\n",
    "\n",
    "\n",
    "Q(s,a): 현재 상태 s에서 행동 a를 취했을 때의 Q-Value\n",
    "\n",
    "α (learning_rate): 학습률, 새로운 정보를 반영하는 비율\n",
    "\n",
    "r (reward): 행동 a를 취한 후 받은 보상\n",
    "\n",
    "γ (discount_factor): 감가율, 미래 보상의 중요도를 조절\n",
    "\n",
    "max(Q(s',a')): 다음 상태 s'에서 취할 수 있는 행동들 중 가장 높은 Q-Value\n",
    "\n",
    "이 업데이트 규칙을 통해 에이전트는 장기적인 관점에서 최선의 행동 정책을 학습하게 됩니다.\n",
    "\n",
    "\n",
    "코드에서는 ε-greedy 방법을 사용하여 행동을 선택합니다. 이는 확률 ε (epsilon)에 따라 무작위 행동을 선택하거나 (탐험, exploration), 현재 Q-Table에서 가장 높은 Q-Value를 가\n",
    "\n",
    "진 행동을 선택하는 (활용, exploitation) 방식입니다. 이를 통해 에이전트는 새로운 가능성을 탐색하면서도 학습한 지식을 활용할 수 있게 됩니다.\n",
    "\n",
    "에피소드를 반복하면서 에이전트는 점진적으로 최적의 행동 정책을 학습하게 되고, 이를 통해 슈퍼 마리오 게임을 잘 플레이할 수 있게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9bb5bd-6209-4e94-b8c1-a7e9241bf651",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Box\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Q-Learning 파라미터\n",
    "learning_rate = 0.1   # 학습률: Q-Value를 업데이트할 때 사용되는 비율. 0에서 1 사이의 값을 가짐.\n",
    "discount_factor = 0.99   # 감가율: 미래의 보상을 현재 가치로 환산할 때 사용되는 비율. 0에서 1 사이의 값을 가짐.\n",
    "epsilon = 0.1   # 탐험을 위한 파라미터: 무작위 행동을 선택할 확률. 0에서 1 사이의 값을 가짐.\n",
    "\n",
    "n_episodes = 1000   # 총 시도할 에피소드 수\n",
    "\n",
    "# Q-Table 초기화. 상태와 액션의 수에 맞게 조정할 필요가 있음\n",
    "# 이 예제에서는 상태와 액션의 수를 간략화하기 위해 임의로 설정함\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])   # Q-Table: 각 상태-행동 쌍의 Q-Value를 저장하는 테이블\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()   # 에피소드를 시작할 때마다 환경을 초기화함\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # ε-greedy 방법으로 행동 선택\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()   # 무작위 행동 선택 (탐험)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])   # Q-Value가 가장 높은 행동 선택 (활용)\n",
    "        \n",
    "        # 환경에서 행동을 취함\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Q-Value 업데이트 (Q-Learning 업데이트 규칙 적용)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state, :])\n",
    "        new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69c66e-818c-4351-8d79-fa44f7ef591b",
   "metadata": {},
   "source": [
    "Super Mario Bros 게임 환경에서 발생하는 AttributeError는 env.observation_space와 env.action_space가 연속적이고 복잡한 공간을 나타내기 때문입니다. Super Mario Bros의 경우, observation_space는 게임 화면의 픽셀 데이터를 나타내며, 이는 매우 큰 차원의 데이터입니다. 따라서, 이러한 환경에서는 전통적인 Q-Table 방식을 직접 적용하기 어렵습니다.\n",
    "\n",
    "대신, Deep Q-Network (DQN)와 같은 방법을 사용하여 복잡한 상태 공간을 처리할 수 있습니다. DQN은 심층 신경망을 사용하여 Q-Value 함수를 근사합니다. 이 접근법은 픽셀 데이터와 같은 고차원 입력을 처리할 수 있으며, 게임과 같은 복잡한 환경에서 효과적입니다.\n",
    "\n",
    "여기서는 DQN을 구현하는 대신, Q-Learning을 Super Mario Bros에 적용하는 기본적인 아이디어를 간략히 설명하겠습니다. 실제 구현을 위해서는 TensorFlow, PyTorch와 같은 라이브러리를 사용하여 DQN을 구현해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003a417-928a-4bb2-9737-82737aa93020",
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "opencv is not install, run `pip install gym[other]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tyron\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\resize_observation.py:61\u001b[0m, in \u001b[0;36mResizeObservation.observation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 학습 과정을 시작합니다.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m---> 41\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# 환경을 초기화하고 첫 관측을 얻습니다.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     state \u001b[38;5;241m=\u001b[39m encode_state(observation)  \u001b[38;5;66;03m# 초기 상태를 얻습니다.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 에피소드 종료 여부를 표시하는 플래그\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tyron\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\frame_stack.py:186\u001b[0m, in \u001b[0;36mFrameStack.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reset the environment with kwargs.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m        The stacked observations\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(obs) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stack)]\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), info\n",
      "File \u001b[1;32mc:\\Users\\Tyron\\anaconda3\\Lib\\site-packages\\gym\\core.py:379\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(obs), info\n",
      "File \u001b[1;32mc:\\Users\\Tyron\\anaconda3\\Lib\\site-packages\\gym\\core.py:380\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(obs), info\n",
      "File \u001b[1;32mc:\\Users\\Tyron\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\resize_observation.py:63\u001b[0m, in \u001b[0;36mResizeObservation.observation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopencv is not install, run `pip install gym[other]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m     )\n\u001b[0;32m     67\u001b[0m observation \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(\n\u001b[0;32m     68\u001b[0m     observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: opencv is not install, run `pip install gym[other]`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "# 환경을 생성하고 조작 가능한 행동을 SIMPLE_MOVEMENT로 설정합니다.\n",
    "# 이는 게임을 단순화하여 7개의 기본 동작만 사용할 수 있게 합니다.\n",
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# 환경의 관측을 84x84 크기의 흑백 이미지로 변환하고, 연속적인 4개의 프레임을 하나의 관측으로 취급합니다.\n",
    "# 이는 에이전트가 시간에 따른 움직임을 인식할 수 있게 해줍니다.\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "# Q-Learning에 사용될 하이퍼파라미터를 설정합니다.\n",
    "learning_rate = 0.1  # Q-Value 업데이트 시 학습률\n",
    "discount_factor = 0.99  # 미래 보상에 대한 할인율\n",
    "epsilon = 0.1  # 탐험을 위한 ε-greedy 전략에서 사용\n",
    "episodes = 1000  # 총 에피소드 수\n",
    "\n",
    "# 상태 공간과 행동 공간의 크기를 설정합니다.\n",
    "# 여기서는 예시로, 상태 공간의 차원과 가능한 행동의 수를 설정합니다.\n",
    "# 실제로는 이 부분이 실제 게임 상태와 행동을 반영해야 합니다.\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# 상태를 인코딩하는 함수입니다. 이 예제에서는 임시 방편으로 상태를 무작위 정수로 변환합니다.\n",
    "# 실제로는 관측된 환경으로부터 유의미한 상태 정보를 추출하는 과정이 필요합니다.\n",
    "def encode_state(observation):\n",
    "    return random.randint(0, num_states - 1)\n",
    "\n",
    "# Q-Table을 0으로 초기화합니다. 여기서는 각 상태와 행동 쌍에 대한 Q-Value를 저장합니다.\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# 학습 과정을 시작합니다.\n",
    "for episode in range(episodes):\n",
    "    observation = env.reset()  # 환경을 초기화하고 첫 관측을 얻습니다.\n",
    "    state = encode_state(observation)  # 초기 상태를 얻습니다.\n",
    "    done = False  # 에피소드 종료 여부를 표시하는 플래그\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # ε-greedy 전략에 따라 탐험합니다.\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # 가장 높은 Q-Value를 가진 행동을 선택합니다.\n",
    "\n",
    "        # 선택된 행동을 수행하고 새로운 상태와 보상을 얻습니다.\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = encode_state(obs)\n",
    "\n",
    "        # Q-Value를 업데이트합니다.\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 보상과 현재 Q-Value를 출력합니다. \n",
    "        print(f\"Episode: {episode}, Reward: {reward}, Q-Value: {q_table[state, action]}\")\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8de0bb-54b0-4224-97ed-4a832ef506da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[state])  \u001b[38;5;66;03m# 활용\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     48\u001b[0m next_state \u001b[38;5;241m=\u001b[39m encode_state(obs[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     50\u001b[0m old_value \u001b[38;5;241m=\u001b[39m q_table[state, action]\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 환경 생성\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Q-Learning 하이퍼파라미터\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 1000\n",
    "\n",
    "# 상태 공간을 단순화하기 위한 이산화 파라미터\n",
    "buckets = (1, 1, 6, 12) # CartPole 상태 변수(위치, 속도, 각도, 각속도)를 이산화하기 위한 구간 수\n",
    "\n",
    "# 행동 공간의 크기 설정\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# 이산화된 상태 공간의 크기 설정\n",
    "num_states = np.prod(buckets)\n",
    "\n",
    "# 상태를 이산화하는 함수\n",
    "def encode_state(observation):\n",
    "    upper_bounds = [env.observation_space.high[2], 0.5]\n",
    "    lower_bounds = [env.observation_space.low[2], -0.5]\n",
    "    ratios = [(observation[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(observation))]\n",
    "    new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(observation))]\n",
    "    new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(new_obs))]\n",
    "    return sum([new_obs[i] * np.prod(buckets[:i]) for i in range(len(new_obs))])\n",
    "\n",
    "# Q-Table 초기화\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# 학습 과정\n",
    "for episode in range(episodes):\n",
    "    observation = env.reset()\n",
    "    state = encode_state(observation[2:4])  # 위치와 속도를 제외한 각도와 각속도만 사용\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # 탐험\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # 활용\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = encode_state(obs[2:4])\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Q-Value 업데이트\n",
    "        q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f21c18-9a7b-42b8-bed4-fb6962913462",
   "metadata": {},
   "source": [
    "### 슈퍼마리오보다 더 간단한 환경들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba071f-0de3-4a6a-8d00-fe681fb38451",
   "metadata": {},
   "source": [
    "gymnasium \n",
    "https://gymnasium.farama.org/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f80069-8a28-4e15-a2b3-9a641785b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[all]\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[all])\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"all\"->gymnasium[all])\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pygame>=2.1.3 (from gymnasium[all])\n",
      "  Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting swig==4.* (from gymnasium[all])\n",
      "  Using cached swig-4.2.1-py2.py3-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
      "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\n",
      "Collecting cython<3 (from gymnasium[all])\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[all])\n",
      "  Using cached mujoco-3.1.4-cp311-cp311-win_amd64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: imageio>=2.14.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (2.33.1)\n",
      "Collecting jax>=0.4.0 (from gymnasium[all])\n",
      "  Using cached jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib>=0.4.0 (from gymnasium[all])\n",
      "  Using cached jaxlib-0.4.26-cp311-cp311-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: lz4>=3.1.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (4.3.2)\n",
      "Collecting opencv-python>=3.0 (from gymnasium[all])\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from gymnasium[all]) (3.8.0)\n",
      "Collecting moviepy>=1.0.0 (from gymnasium[all])\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting torch>=1.0.0 (from gymnasium[all])\n",
      "  Using cached torch-2.2.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from imageio>=2.14.1->gymnasium[all]) (10.2.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.4.0->gymnasium[all])\n",
      "  Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from jax>=0.4.0->gymnasium[all])\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from jax>=0.4.0->gymnasium[all]) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy>=1.0.0->gymnasium[all])\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\n",
      "Collecting proglog<=1.0.0 (from moviepy>=1.0.0->gymnasium[all])\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting imageio-ffmpeg>=0.2.0 (from moviepy>=1.0.0->gymnasium[all])\n",
      "  Using cached imageio_ffmpeg-0.4.9-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting absl-py (from mujoco>=2.3.3->gymnasium[all])\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting etils[epath] (from mujoco>=2.3.3->gymnasium[all])\n",
      "  Using cached etils-1.8.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\n",
      "  Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[all])\n",
      "  Using cached PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: cffi>=1.10 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\n",
      "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"all\"->gymnasium[all])\n",
      "  Using cached ale_py-0.8.1-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from torch>=1.0.0->gymnasium[all]) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from torch>=1.0.0->gymnasium[all]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from torch>=1.0.0->gymnasium[all]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from torch>=1.0.0->gymnasium[all]) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from torch>=1.0.0->gymnasium[all]) (2023.10.0)\n",
      "Collecting importlib-resources (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"all\"->gymnasium[all])\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (68.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy>=1.0.0->gymnasium[all]) (0.4.6)\n",
      "Requirement already satisfied: zipp in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[all]) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tyron\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0.0->gymnasium[all]) (1.3.0)\n",
      "Using cached swig-4.2.1-py2.py3-none-win_amd64.whl (2.6 MB)\n",
      "Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Using cached jax-0.4.26-py3-none-any.whl (1.9 MB)\n",
      "Using cached jaxlib-0.4.26-cp311-cp311-win_amd64.whl (46.5 MB)\n",
      "Using cached mujoco-3.1.4-cp311-cp311-win_amd64.whl (3.9 MB)\n",
      "Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Using cached torch-2.2.2-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "Using cached ale_py-0.8.1-cp311-cp311-win_amd64.whl (952 kB)\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-win_amd64.whl (493 kB)\n",
      "Using cached imageio_ffmpeg-0.4.9-py3-none-win_amd64.whl (22.6 MB)\n",
      "Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "Using cached etils-1.8.0-py3-none-any.whl (156 kB)\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [16 lines of output]\n",
      "      Using setuptools (version 68.2.2).\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      error: command 'swig.exe' failed: None\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip3 install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe4759-b329-49f5-9c3d-badf916f27f5",
   "metadata": {},
   "source": [
    "CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2755705e-cb16-4189-9398-9e8e3e2da2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym as gym\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "env.reset()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85d9732c-a8a1-4112-b911-2bd71c54b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 18.0\n",
      "Episode: 1, Total Reward: 20.0\n",
      "Episode: 2, Total Reward: 17.0\n",
      "Episode: 3, Total Reward: 39.0\n",
      "Episode: 4, Total Reward: 49.0\n",
      "Episode: 5, Total Reward: 35.0\n",
      "Episode: 6, Total Reward: 36.0\n",
      "Episode: 7, Total Reward: 27.0\n",
      "Episode: 8, Total Reward: 16.0\n",
      "Episode: 9, Total Reward: 16.0\n",
      "Episode: 10, Total Reward: 28.0\n",
      "Episode: 11, Total Reward: 11.0\n",
      "Episode: 12, Total Reward: 40.0\n",
      "Episode: 13, Total Reward: 14.0\n",
      "Episode: 14, Total Reward: 49.0\n",
      "Episode: 15, Total Reward: 39.0\n",
      "Episode: 16, Total Reward: 13.0\n",
      "Episode: 17, Total Reward: 18.0\n",
      "Episode: 18, Total Reward: 38.0\n",
      "Episode: 19, Total Reward: 37.0\n",
      "Episode: 20, Total Reward: 46.0\n",
      "Episode: 21, Total Reward: 28.0\n",
      "Episode: 22, Total Reward: 11.0\n",
      "Episode: 23, Total Reward: 30.0\n",
      "Episode: 24, Total Reward: 10.0\n",
      "Episode: 25, Total Reward: 32.0\n",
      "Episode: 26, Total Reward: 18.0\n",
      "Episode: 27, Total Reward: 22.0\n",
      "Episode: 28, Total Reward: 26.0\n",
      "Episode: 29, Total Reward: 48.0\n",
      "Episode: 30, Total Reward: 21.0\n",
      "Episode: 31, Total Reward: 34.0\n",
      "Episode: 32, Total Reward: 23.0\n",
      "Episode: 33, Total Reward: 13.0\n",
      "Episode: 34, Total Reward: 17.0\n",
      "Episode: 35, Total Reward: 36.0\n",
      "Episode: 36, Total Reward: 50.0\n",
      "Episode: 37, Total Reward: 26.0\n",
      "Episode: 38, Total Reward: 18.0\n",
      "Episode: 39, Total Reward: 27.0\n",
      "Episode: 40, Total Reward: 15.0\n",
      "Episode: 41, Total Reward: 23.0\n",
      "Episode: 42, Total Reward: 14.0\n",
      "Episode: 43, Total Reward: 40.0\n",
      "Episode: 44, Total Reward: 20.0\n",
      "Episode: 45, Total Reward: 25.0\n",
      "Episode: 46, Total Reward: 11.0\n",
      "Episode: 47, Total Reward: 50.0\n",
      "Episode: 48, Total Reward: 26.0\n",
      "Episode: 49, Total Reward: 14.0\n",
      "Episode: 50, Total Reward: 16.0\n",
      "Episode: 51, Total Reward: 14.0\n",
      "Episode: 52, Total Reward: 66.0\n",
      "Episode: 53, Total Reward: 38.0\n",
      "Episode: 54, Total Reward: 32.0\n",
      "Episode: 55, Total Reward: 14.0\n",
      "Episode: 56, Total Reward: 13.0\n",
      "Episode: 57, Total Reward: 14.0\n",
      "Episode: 58, Total Reward: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyron\\AppData\\Local\\Temp\\ipykernel_13644\\1151373085.py:50: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
      "C:\\Users\\Tyron\\AppData\\Local\\Temp\\ipykernel_13644\\1151373085.py:50: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 59, Total Reward: 27.0\n",
      "Episode: 60, Total Reward: 29.0\n",
      "Episode: 61, Total Reward: 20.0\n",
      "Episode: 62, Total Reward: 13.0\n",
      "Episode: 63, Total Reward: 14.0\n",
      "Episode: 64, Total Reward: 25.0\n",
      "Episode: 65, Total Reward: 47.0\n",
      "Episode: 66, Total Reward: 15.0\n",
      "Episode: 67, Total Reward: 14.0\n",
      "Episode: 68, Total Reward: 10.0\n",
      "Episode: 69, Total Reward: 20.0\n",
      "Episode: 70, Total Reward: 16.0\n",
      "Episode: 71, Total Reward: 33.0\n",
      "Episode: 72, Total Reward: 15.0\n",
      "Episode: 73, Total Reward: 14.0\n",
      "Episode: 74, Total Reward: 12.0\n",
      "Episode: 75, Total Reward: 20.0\n",
      "Episode: 76, Total Reward: 12.0\n",
      "Episode: 77, Total Reward: 13.0\n",
      "Episode: 78, Total Reward: 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyron\\AppData\\Local\\Temp\\ipykernel_13644\\1151373085.py:50: RuntimeWarning: invalid value encountered in scalar add\n",
      "  q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 79, Total Reward: 28.0\n",
      "Episode: 80, Total Reward: 11.0\n",
      "Episode: 81, Total Reward: 68.0\n",
      "Episode: 82, Total Reward: 23.0\n",
      "Episode: 83, Total Reward: 19.0\n",
      "Episode: 84, Total Reward: 13.0\n",
      "Episode: 85, Total Reward: 19.0\n",
      "Episode: 86, Total Reward: 12.0\n",
      "Episode: 87, Total Reward: 12.0\n",
      "Episode: 88, Total Reward: 14.0\n",
      "Episode: 89, Total Reward: 10.0\n",
      "Episode: 90, Total Reward: 11.0\n",
      "Episode: 91, Total Reward: 13.0\n",
      "Episode: 92, Total Reward: 12.0\n",
      "Episode: 93, Total Reward: 16.0\n",
      "Episode: 94, Total Reward: 22.0\n",
      "Episode: 95, Total Reward: 11.0\n",
      "Episode: 96, Total Reward: 19.0\n",
      "Episode: 97, Total Reward: 11.0\n",
      "Episode: 98, Total Reward: 27.0\n",
      "Episode: 99, Total Reward: 11.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 환경을 생성합니다.\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "# Q-Learning에 사용될 하이퍼파라미터를 설정합니다.\n",
    "learning_rate = 3  # Q-Value 업데이트 시 학습률\n",
    "discount_factor = 7  # 미래 보상에 대한 할인율\n",
    "epsilon = 0.6  # 탐험을 위한 ε-greedy 전략에서 사용\n",
    "episodes = 100  # 총 에피소드 수\n",
    "\n",
    "# 상태 공간과 행동 공간의 크기를 설정합니다.\n",
    "num_states = env.observation_space.shape[0]  # 상태 공간의 차원\n",
    "num_actions = env.action_space.n  # 가능한 행동의 수\n",
    "\n",
    "# 상태를 인코딩하는 함수입니다.\n",
    "# 이 예제에서는 CartPole 환경의 상태를 양자화(quantization)하여 이산 상태로 변환합니다.\n",
    "\n",
    "def encode_state(observation):\n",
    "    state = 0\n",
    "    for i, obs in enumerate(observation):\n",
    "        state += (obs + 2.4) // 0.25 * 4 ** i\n",
    "    return int(state)\n",
    "\n",
    "q_table = np.zeros((4096, num_actions))  # 4^4 = 4096\n",
    "\n",
    "\n",
    "# 학습 과정을 시작합니다.\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset()  # 환경을 초기화하고 첫 관측을 얻습니다.\n",
    "    state = encode_state(observation)  # 초기 상태를 얻습니다.\n",
    "    done = False  # 에피소드 종료 여부를 표시하는 플래그\n",
    "    total_reward = 0  # 에피소드의 총 보상을 저장할 변수\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # ε-greedy 전략에 따라 탐험합니다.\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # 가장 높은 Q-Value를 가진 행동을 선택합니다.\n",
    "\n",
    "        # 선택된 행동을 수행하고 새로운 상태와 보상을 얻습니다.\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = encode_state(obs)\n",
    "\n",
    "        # Q-Value를 업데이트합니다.\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # 에피소드마다 총 보상을 출력합니다.\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9eb957b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m     31\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# 환경을 초기화하고 첫 관측을 얻습니다.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     state \u001b[38;5;241m=\u001b[39m encode_state(observation)  \u001b[38;5;66;03m# 초기 상태를 얻습니다.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 에피소드 종료 여부를 표시하는 플래그\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 에피소드의 총 보상을 저장할 변수\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 23\u001b[0m, in \u001b[0;36mencode_state\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m     21\u001b[0m buckets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# 각 상태 변수를 양자화할 구간의 개수\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(observation):\n\u001b[1;32m---> 23\u001b[0m     state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (obs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.4\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m4.8\u001b[39m \u001b[38;5;241m/\u001b[39m buckets[i]) \u001b[38;5;241m*\u001b[39m (buckets[i] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(state)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 환경을 생성합니다.\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Q-Learning에 사용될 하이퍼파라미터를 설정합니다.\n",
    "learning_rate = 0.1  # Q-Value 업데이트 시 학습률\n",
    "discount_factor = 0.99  # 미래 보상에 대한 할인율\n",
    "epsilon = 0.4  # 탐험을 위한 ε-greedy 전략에서 사용\n",
    "episodes = 50  # 총 에피소드 수\n",
    "\n",
    "# 상태 공간과 행동 공간의 크기를 설정합니다.\n",
    "num_states = env.observation_space.shape[0]  # 상태 공간의 차원\n",
    "num_actions = env.action_space.n  # 가능한 행동의 수\n",
    "\n",
    "# 상태를 인코딩하고 이산 상태로 변환하는 함수입니다.\n",
    "def encode_state(observation):\n",
    "    state = 0\n",
    "    buckets = [1, 1, 6, 3]  # 각 상태 변수를 양자화할 구간의 개수\n",
    "    for i, obs in enumerate(observation):\n",
    "        state += (obs + 2.4) // (4.8 / buckets[i]) * (buckets[i] ** i)\n",
    "    return int(state)\n",
    "\n",
    "# Q-테이블을 초기화합니다.\n",
    "q_table = np.zeros((buckets[0] * buckets[1] * buckets[2] * buckets[3], num_actions))\n",
    "\n",
    "# 학습 과정을 시작합니다.\n",
    "for episode in range(episodes):\n",
    "    observation = env.reset()  # 환경을 초기화하고 첫 관측을 얻습니다.\n",
    "    state = encode_state(observation)  # 초기 상태를 얻습니다.\n",
    "    done = False  # 에피소드 종료 여부를 표시하는 플래그\n",
    "    total_reward = 0  # 에피소드의 총 보상을 저장할 변수\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # ε-greedy 전략에 따라 탐험합니다.\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # 가장 높은 Q-Value를 가진 행동을 선택합니다.\n",
    "\n",
    "        # 선택된 행동을 수행하고 새로운 상태와 보상을 얻습니다.\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state(next_observation)\n",
    "\n",
    "        # Q-Value를 업데이트합니다.\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # 에피소드마다 총 보상을 출력합니다.\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # 200을 넘으면 종료합니다.\n",
    "    if total_reward >= 200:\n",
    "        print(\"Success! Total reward exceeds 200.\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77472f77-6057-48b1-af13-f9a0d8077e4d",
   "metadata": {},
   "source": [
    "### 연습! \n",
    "Frozen Lake 환경에서 똑똑한 인공지능 에이전트를 만들어보세요. \n",
    "ChatGPT, Claude, Google Gemini 와 같은 LLM을 사용해보시면서 Q-learning을 구현해보고 아래의 코드를 더 효율적이고 효과적으로 바꾸보시는 연습을 해보시기 바랍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac66b5b3-9f67-4a0f-a570-bf6e7661b9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[state])  \u001b[38;5;66;03m# 가장 높은 Q-Value를 가진 행동을 선택합니다.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 선택된 행동을 수행하고 새로운 상태와 보상을 얻습니다.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Q-Value를 업데이트합니다.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m old_value \u001b[38;5;241m=\u001b[39m q_table[state, action]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/toy_text/frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/toy_text/frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/toy_text/frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    371\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    372\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    376\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 환경을 생성합니다.\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "\n",
    "# Q-Learning에 사용될 하이퍼파라미터를 설정합니다.\n",
    "learning_rate = 0.8  # Q-Value 업데이트 시 학습률\n",
    "discount_factor = 0.95  # 미래 보상에 대한 할인율\n",
    "epsilon = 0.1  # 탐험을 위한 ε-greedy 전략에서 사용\n",
    "episodes = 2000  # 총 에피소드 수\n",
    "\n",
    "# 상태 공간과 행동 공간의 크기를 설정합니다.\n",
    "num_states = env.observation_space.n  # 상태 공간의 크기\n",
    "num_actions = env.action_space.n  # 가능한 행동의 수\n",
    "\n",
    "# Q-Table을 0으로 초기화합니다.\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# 학습 과정을 시작합니다.\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]  # 환경을 초기화하고 초기 상태를 얻습니다.\n",
    "    done = False  # 에피소드 종료 여부를 표시하는 플래그\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # ε-greedy 전략에 따라 탐험합니다.\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # 가장 높은 Q-Value를 가진 행동을 선택합니다.\n",
    "        \n",
    "        # 선택된 행동을 수행하고 새로운 상태와 보상을 얻습니다.\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Q-Value를 업데이트합니다.\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # 100 에피소드마다 현재 에피소드 번호를 출력합니다.\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "# 학습이 완료된 후, 최종 정책을 출력합니다.\n",
    "print(\"Final Q-Table:\")\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
